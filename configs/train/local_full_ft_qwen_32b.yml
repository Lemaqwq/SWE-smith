# Config for local multi-device full finetuning
# Identical hyperparameters to the Modal cloud version for reproducibility
exp_name: qwen2p5-coder-32b-full-lr1e-4-warmup5___xml_all_250413
output_dir: /mnt/nfs/liheng/models/exps/${exp_name}

# Model Arguments
model:
  _component_: torchtune.models.qwen2_5.qwen2_5_32b_instruct

tokenizer:
  _component_: torchtune.models.qwen2_5.qwen2_5_tokenizer
  # PATH CHANGE: Updated to your local NFS path
  path: /mnt/nfs/liheng/models/Qwen/Qwen2.5-Coder-32B-Instruct/vocab.json
  merges_file: /mnt/nfs/liheng/models/Qwen/Qwen2.5-Coder-32B-Instruct/merges.txt
  max_seq_len: 32768


# 1. Change this to True
resume_from_checkpoint: True

checkpointer:
  _component_: torchtune.training.FullModelHFCheckpointer
  # 2. Update checkpoint_dir to point to the epoch_0 folder
  checkpoint_dir: /mnt/nfs/liheng/models/Qwen/Qwen2.5-Coder-32B-Instruct
  # 3. List the model files found in that specific folder
  checkpoint_files: [
    model-00001-of-00014.safetensors,
    model-00002-of-00014.safetensors,
    model-00003-of-00014.safetensors,
    model-00004-of-00014.safetensors,
    model-00005-of-00014.safetensors,
    model-00006-of-00014.safetensors,
    model-00007-of-00014.safetensors,
    model-00008-of-00014.safetensors,
    model-00009-of-00014.safetensors,
    model-00010-of-00014.safetensors,
    model-00011-of-00014.safetensors,
    model-00012-of-00014.safetensors,
    model-00013-of-00014.safetensors,
    model-00014-of-00014.safetensors,
  ]
  
  # 4. Point this to the recipe_state.pt file
  recipe_checkpoint: recipe_state/recipe_state.pt
  
  output_dir: /mnt/nfs/liheng/models/exps/${exp_name}
  model_type: QWEN2

# checkpointer:
#   _component_: torchtune.training.FullModelHFCheckpointer
#   # PATH CHANGE: Updated to your local NFS path
#   checkpoint_dir: /mnt/nfs/liheng/models/Qwen/Qwen2.5-Coder-32B-Instruct
#   checkpoint_files: [
#     model-00001-of-00014.safetensors,
#     model-00002-of-00014.safetensors,
#     model-00003-of-00014.safetensors,
#     model-00004-of-00014.safetensors,
#     model-00005-of-00014.safetensors,
#     model-00006-of-00014.safetensors,
#     model-00007-of-00014.safetensors,
#     model-00008-of-00014.safetensors,
#     model-00009-of-00014.safetensors,
#     model-00010-of-00014.safetensors,
#     model-00011-of-00014.safetensors,
#     model-00012-of-00014.safetensors,
#     model-00013-of-00014.safetensors,
#     model-00014-of-00014.safetensors,
#   ]
#   recipe_checkpoint: null
#   output_dir: ${output_dir}
#   model_type: QWEN2
#   safe_serialization: True
# resume_from_checkpoint: False

# Dataset and Sampler
dataset:
  _component_: torchtune.datasets.chat_dataset
  source: json
  # PATH CHANGE: Point this to your local .jsonl location
  data_files: /mnt/nfs/liheng/data/SWE-agent-LM-32B_train_5016_trajectories.json
  split: train
  conversation_column: messages
  conversation_style: openai
  train_on_input: False
  new_system_prompt: null
  packed: False  # Kept False to match original exactly

# Identical Hyperparameters
seed: 42
shuffle: True
batch_size: 1

# Optimizer and Scheduler
optimizer:
  _component_: torch.optim.AdamW
  fused: True
  weight_decay: 0.01
  lr: 1e-4
lr_scheduler:
  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup
  num_warmup_steps: 5
optimizer_in_bwd: True
loss:
  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss

# Training
epochs: 3
max_steps_per_epoch: null
gradient_accumulation_steps: 1 
compile: True  

# Logging
metric_logger:
  _component_: torchtune.training.metric_logging.WandBLogger
  project: swesmith
  group: ${exp_name}
  job_type: full_finetune_distributed
log_every_n_steps: 1
log_peak_memory_stats: True

# Environment
device: cuda
dtype: bf16
enable_activation_checkpointing: True
enable_activation_offloading: False

# Profiler (Disabled as per original)
profiler:
  _component_: torchtune.training.setup_torch_profiler
  enabled: False
  output_dir: ${output_dir}/profiling_outputs
  cpu: True
  cuda: True
  profile_memory: False
  with_stack: False
  record_shapes: True
  with_flops: False
  wait_steps: 5
  warmup_steps: 5
  active_steps: 2
  num_cycles: 1